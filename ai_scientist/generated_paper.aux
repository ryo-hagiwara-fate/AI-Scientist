\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{haarnoja2018soft}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\citation{mnih2016asynchronous}
\citation{mnih2015human}
\citation{van2016deep}
\citation{lillicrap2016continuous}
\@writefile{toc}{\contentsline {section}{\numberline {2}Main Body}{2}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor}{2}{subsection.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Asynchronous Methods for Deep Reinforcement Learning}{2}{subsection.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Human-level Control Through Deep Reinforcement Learning}{2}{subsection.2.3}\protected@file@percent }
\citation{van2016deep}
\citation{wang2016dueling}
\citation{christiano2017deep}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Continuous Control with Deep Reinforcement Learning}{3}{subsection.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Deep Reinforcement Learning with Double Q-Learning}{3}{subsection.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6}Dueling Network Architectures for Deep Reinforcement Learning}{3}{subsection.2.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.7}Deep Reinforcement Learning from Human Preferences}{3}{subsection.2.7}\protected@file@percent }
\citation{hessel2018rainbow}
\bibstyle{plainnat}
\bibdata{references}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.8}Rainbow: Combining Improvements in Deep Reinforcement Learning}{4}{subsection.2.8}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Conclusion}{4}{section.3}\protected@file@percent }
\gdef \@abspage@last{4}
